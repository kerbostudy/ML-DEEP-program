{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html 코드 분석을 위한 bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 웹을 동적으로 움직이기 위한 selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# csv 파일 작성을 위한 pandas\n",
    "import pandas as pd\n",
    "\n",
    "# 날짜관련 작업을 위한 datetime\n",
    "import datetime\n",
    "\n",
    "# 파일 경로 설정을 위한 os\n",
    "import os\n",
    "\n",
    "# 콘솔에 입력한 데이터가 암호화 처리\n",
    "import getpass\n",
    "\n",
    "# 프로그램 일시정지를 위한  time\n",
    "import time\n",
    "\n",
    "# 입력한 주소로 요청을 보내기 위한 requests 라이브러리리\n",
    "import requests\n",
    "\n",
    "# 정규표현식을 위한 re\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_browser = True  # 크롬 브라우저 창 표시 여부\n",
    "\n",
    "# 원하는 검색어 설정\n",
    "search_keyword = \"machine learning\" \n",
    "\n",
    "# 작업 디렉토리까지의 절대경로 입력 + 마지막에 / 입력\n",
    "load_folder = \"C:/Users/User/Desktop/web2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL에 KCI 우수등재, KCI 등재, KCI 후보등재 선택이 되어있음음\n",
    "URL_TEMPLATE = (\n",
    "    f\"https://www.riss.kr/search/Search.do?\" # RISS 검색 페이지의 기본 URL\n",
    "    f\"isDetailSearch=N&searchGubun=true&viewYn=OP&queryText=&strQuery={search_keyword}\"\n",
    "    f\"&exQuery=regnm%3AKCI%EB%93%B1%EC%9E%AC%E2%97%88regnm%3AKCI%EC%9A%B0%EC%88%98%EB%93%B1%EC%9E%AC%E2%97%88\"\n",
    "    f\"regnm%3AKCI%EB%93%B1%EC%9E%AC%ED%9B%84%EB%B3%B4%E2%97%88pyear%3A2024%E2%97%88pyear%3A2023%E2%97%88\"\n",
    "    f\"pyear%3A2022%E2%97%88pyear%3A2021%E2%97%88pyear%3A2020%E2%97%88pyear%3A2019%E2%97%88pyear%3A2018\"\n",
    "    f\"%E2%97%88pyear%3A2017%E2%97%88pyear%3A2016%E2%97%88pyear%3A2015%E2%97%88\"\n",
    "    f\"&exQueryText=%EB%93%B1%EC%9E%AC%EC%A0%95%EB%B3%B4+%5BKCI%EB%93%B1%EC%9E%AC%5D%40%40regnm%3AKCI%EB%93%B1\"\n",
    "    f\"%EC%9E%AC%E2%97%88%EB%93%B1%EC%9E%AC%EC%A0%95%EB%B3%B4+%5BKCI%EC%9A%B0%EC%88%98%EB%93%B1%EC%9E%AC%5D\"\n",
    "    f\"%40%40regnm%3AKCI%EC%9A%B0%EC%88%98%EB%93%B1%EC%9E%AC%E2%97%88%EB%93%B1%EC%9E%AC%EC%A0%95%EB%B3%B4\"\n",
    "    f\"+%5BKCI%EB%93%B1%EC%9E%AC%ED%9B%84%EB%B3%B4%5D%40%40regnm%3AKCI%EB%93%B1%EC%9E%AC%ED%9B%84%EB%B3%B4\"\n",
    "    f\"%E2%97%88%EB%B0%9C%ED%96%89%EC%97%B0%EB%8F%84+%5B2024%5D%40%40pyear%3A2024%E2%97%88%EB%B0%9C%ED%96%89\"\n",
    "    f\"%EC%97%B0%EB%8F%84+%5B2023%5D%40%40pyear%3A2023%E2%97%88%EB%B0%9C%ED%96%89%EC%97%B0%EB%8F%84+%5B2022%5D\"\n",
    "    f\"%40%40pyear%3A2022%E2%97%88%EB%B0%9C%ED%96%89%EC%97%B0%EB%8F%84+%5B2021%5D%40%40pyear%3A2021%E2%97%88\"\n",
    "    f\"%EB%B0%9C%ED%96%89%EC%97%B0%EB%8F%84+%5B2020%5D%40%40pyear%3A2020%E2%97%88%EB%B0%9C%ED%96%89%EC%97%B0\"\n",
    "    f\"%EB%8F%84+%5B2019%5D%40%40pyear%3A2019%E2%97%88%EB%B0%9C%ED%96%89%EC%97%B0%EB%8F%84+%5B2018%5D%40%40\"\n",
    "    f\"pyear%3A2018%E2%97%88%EB%B0%9C%ED%96%89%EC%97%B0%EB%8F%84+%5B2017%5D%40%40pyear%3A2017%E2%97%88%EB\"\n",
    "    f\"%B0%9C%ED%96%89%EC%97%B0%EB%8F%84+%5B2016%5D%40%40pyear%3A2016%E2%97%88%EB%B0%9C%ED%96%89%EC%97%B0%EB\"\n",
    "    f\"%8F%84+%5B2015%5D%40%40pyear%3A2015%E2%97%88&order=%2FDESC&onHanja=false&strSort=RANK&p_year1=&p_year2=\"\n",
    "    f\"&iStartCount={{iStartCount}}\"\n",
    "    f\"&orderBy=&mat_type=&mat_subtype=&fulltext_kind=&t_gubun=&learning_type=&ccl_code=&inside_outside=&fric_yn=&db_type=&image_yn=\"\n",
    "    f\"&gubun=&kdc=&ttsUseYn=&l_sub_code=&fsearchMethod=search&sflag=1&isFDetailSearch=N&pageNumber=1\"\n",
    "    f\"&resultKeyword={search_keyword}&fsearchSort=&fsearchOrder=&limiterList=&limiterListText=&facetList=&facetListText=&fsearchDB=\"\n",
    "    f\"&icate=re_a_kor&colName=re_a_kor&pageScale=10&isTab=Y&regnm=&dorg_storage=&language=&language_code=&clickKeyword=&relationKeyword=\"\n",
    "    f\"&query={search_keyword}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롬에서 관리자 모드로 들어가서 하나하나 확인\n",
    "result = []  # 결과를 저장할 리스트 초기화\n",
    "base_url = \"https://www.riss.kr/search/Search.do?\"  # RISS 검색 기본 URL\n",
    "response = requests.get(f\"{base_url}isDetailSearch=N&searchGubun=true&viewYn=OP&queryText=&strQuery={search_keyword}\")  # 기본 검색 페이지 요청\n",
    "soup = BeautifulSoup(response.text, 'html.parser')  # BeautifulSoup 객체 생성\n",
    "\n",
    "for page in range(2):  # 2페이지까지 크롤링 (페이지당 10개 결과)\n",
    "    iStartCount = (page - 1) * 10  # 페이지 시작 결과 인덱스 계산\n",
    "    url = URL_TEMPLATE.format(iStartCount=iStartCount)  # 페이지별 URL 생성\n",
    "    res = requests.get(url)  # 페이지별 URL 요청\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")  # BeautifulSoup 객체 생성\n",
    "    thesis = soup.find('div', attrs={\"class\": \"srchResultListW\"})  # 검색 결과 리스트를 감싸는 div 태그 찾기\n",
    "\n",
    "    for i in thesis.find_all('div', attrs={'class': 'cont ml60'}):  # 각 검색 결과 항목을 순회\n",
    "        title_tag = i.find('a')  # 제목 태그 찾기\n",
    "        title = title_tag.get_text(strip=True)  # 제목 추출 및 공백 제거\n",
    "        info = i.find('p', attrs={'class': 'etc'})  # 정보(날짜, 저자 등)를 담고 있는 p 태그 찾기\n",
    "        year = [i.get_text(strip=True) for i in info.find_all('span')][2]  # 날짜 추출\n",
    "        authors_list = [a.get_text(strip=True) for a in info.find_all('span', attrs={'class': 'writer'})]  # 저자 목록 추출\n",
    "        authors = ', '.join(authors_list)  # 여러 저자를 쉼표로 구분하여 하나의 문자열로 합침\n",
    "        abstract = i.find('p', attrs={'class': 'preAbstract'}).get_text(strip=True) if i.find('p', attrs={'class': 'preAbstract'}) else '' # 초록 추출, 없는 경우 빈 문자열 할당\n",
    "\n",
    "        result.append([title, year, authors, abstract])  # 결과 리스트에 추가\n",
    "\n",
    "# 결과를 DataFrame으로 변환\n",
    "df = pd.DataFrame(result, columns=['제목', '날짜', '저자', 'abstract']) # DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # df의 요약정보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜에서 연도만 추출하여 '날짜' 컬럼에 저장\n",
    "# dbpia에서 가지고 오면 연도와 월까지 나와있어서 연도만 추출\n",
    "# riss는 연도만 나와있음\n",
    "df['날짜'] = df['날짜'].str.split('-', expand=True)[0]  # '날짜' 컬럼의 문자열을 '-'를 기준으로 분리하고 첫 번째 요소를 연도로로 저장\n",
    "\n",
    "# 결과 출력\n",
    "df.head(10)  # DataFrame의 처음 10개 행을 출력하여 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목 확인하고 싶으면 확인\n",
    "# df_title = df['제목']  # DataFrame에서 '제목' 컬럼을 추출하여 df_title 변수에 저장\n",
    "# df_title.head(10)  # df_title 변수에 저장된 제목 데이터 중 처음 10개의 행을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(title):\n",
    "    # 모든 공백 제거\n",
    "    title = re.sub(r'\\s+', '', title)  # 정규표현식을 사용하여 모든 종류의 공백 제거\n",
    "    return title\n",
    "\n",
    "# 데이터에 대해 제목 정리 적용\n",
    "df['제목'] = df['제목'].apply(clean_title)  # DataFrame의 '제목' 컬럼에 clean_title 함수를 적용하여 공백 제거\n",
    "\n",
    "# 결과 출력\n",
    "print(df)  # 공백이 제거된 제목이 포함된 DataFrame 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_keyword를 넣어서 csv 파일로 저장\n",
    "df.to_csv(f'riss_crawling_{search_keyword}.csv', index=False)  # DataFrame을 CSV 파일로 저장, 인덱스 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 페이지를 돌면서 링크를 가져오는 함수\n",
    "def get_paper_links(driver, url):\n",
    "    driver.get(url)  # 주어진 URL로 웹 페이지를 엽니다.\n",
    "    time.sleep(2)  # 페이지 로딩을 2초 동안 기다립니다.\n",
    "\n",
    "    soup = bs(driver.page_source, 'html.parser')  # 페이지 소스를 BeautifulSoup으로 파싱합니다.\n",
    "    links = []  # 링크를 저장할 리스트를 초기화합니다.\n",
    "\n",
    "    # CSS 선택자를 사용하여 논문 링크를 추출합니다.\n",
    "    for a in soup.select(\"#divContent > div > div.rightContent.wd756 > div > div.srchResultW > div.srchResultListW > ul > li > div.cont.ml60 > p.title > a\"):\n",
    "        links.append('https://www.riss.kr' + a.get('href', ''))  # 추출된 링크를 리스트에 추가합니다.\n",
    "\n",
    "    return links  # 추출된 링크 리스트를 반환합니다.\n",
    "\n",
    "# 제목과 키워드를 가져오는 함수\n",
    "def get_paper_details(driver, url):\n",
    "    driver.get(url)  # 주어진 URL로 웹 페이지를 엽니다.\n",
    "    time.sleep(1.5)  # 페이지 로딩을 1.5초 동안 기다립니다.\n",
    "\n",
    "    soup = bs(driver.page_source, 'html.parser')  # 페이지 소스를 BeautifulSoup으로 파싱합니다.\n",
    "\n",
    "    title = soup.select_one('h3.title')  # CSS 선택자를 사용하여 제목을 추출합니다.\n",
    "    title_kor = title.get_text(strip=True) if title else \"\"  # 제목이 존재하는 경우 텍스트를 추출하고, 없는 경우 빈 문자열을 할당합니다.\n",
    "    \n",
    "    keywords = soup.select_one('#thesisInfoDiv > div.infoDetail.on > div.infoDetailL > ul > li:nth-of-type(7) > div > p')  # CSS 선택자를 사용하여 키워드를 추출합니다.\n",
    "    keywords_text = keywords.get_text(strip=True) if keywords else \"\"  # 키워드가 존재하는 경우 텍스트를 추출하고, 없는 경우 빈 문자열을 할당합니다.\n",
    "\n",
    "    return {  # 제목과 키워드를 딕셔너리 형태로 반환합니다.\n",
    "        '제목': title_kor,\n",
    "        '키워드': keywords_text,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  # 스크립트가 직접 실행될 때만 실행되는 코드 블록\n",
    "    driver_path = 'chromedriver-win64/chromedriver-win64/chromedriver.exe'  # Chrome WebDriver 실행 파일 경로 설정\n",
    "    chrome_service = Service(driver_path)  # Chrome WebDriver 서비스 객체 생성\n",
    "\n",
    "    # 크롬 옵션 설정\n",
    "    chrome_options = webdriver.ChromeOptions()  # Chrome WebDriver 옵션 객체 생성\n",
    "    \n",
    "    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)  # Chrome WebDriver 객체 생성\n",
    "\n",
    "    user_name = getpass.getuser()  # 현재 사용자 이름 가져오기\n",
    "    folder_root = load_folder  # 파일 저장 경로 설정 (load_folder는 이전에 정의된 변수)\n",
    "\n",
    "    details = []  # 논문 상세 정보를 저장할 리스트 초기화\n",
    "\n",
    "    for page in range(1, 3):  # 1페이지부터 2페이지까지 반복 (페이지당 10개 결과)\n",
    "        iStartCount = (page - 1) * 10  # 페이지 시작 결과 인덱스 계산\n",
    "        # pageNumber, iStartCount 값에 맞춰 URL 생성\n",
    "        url = URL_TEMPLATE.format(iStartCount=iStartCount)  # 페이지별 URL 생성 (URL_TEMPLATE은 이전에 정의된 템플릿)\n",
    "\n",
    "        paper_links = get_paper_links(driver, url)  # 현재 페이지의 논문 링크 추출\n",
    "\n",
    "        for link in paper_links:  # 각 논문 링크에 대해 반복\n",
    "            paper_details = get_paper_details(driver, link)  # 논문 상세 정보 추출\n",
    "            details.append(paper_details)  # 추출된 정보를 리스트에 추가\n",
    "\n",
    "    driver.quit()  # WebDriver 종료\n",
    "    print(details)  # 추출된 모든 논문 상세 정보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(title):\n",
    "    # 줄 바꿈 문자(\\n)와 탭 문자(\\t) 제거 및 '=' 뒤의 텍스트 제거\n",
    "    title = re.sub(r'[\\n\\t]+', ' ', title)  # \\n과 \\t을 공백으로 대체\n",
    "    title = re.sub(r'=\\s*.*$', '', title)  # '=' 뒤의 텍스트 제거\n",
    "    title = re.sub(r'\\s+', ' ', title)  # 여러 개의 공백을 하나로 줄임\n",
    "    title = re.sub(r'\\s+', '', title)  # 모든 공백 제거\n",
    "    return title\n",
    "\n",
    "# 데이터에 대해 제목 정리 적용\n",
    "for data in details:\n",
    "    data['제목'] = clean_title(data['제목'])  # 각 논문 데이터의 '제목'에 clean_title 함수 적용\n",
    "\n",
    "# 결과 출력\n",
    "print(details)  # 정제된 제목이 포함된 논문 데이터 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_authors(keywords):\n",
    "    # 키워드를 세미콜론으로 구분하여 리스트로 변환\n",
    "    keywords_list = keywords.split(';')  # 세미콜론(;)을 기준으로 키워드 문자열을 분리하여 리스트로 저장\n",
    "    formatted_keywords = [keyword.strip() for keyword in keywords_list]  # 각 키워드의 앞뒤 공백을 제거\n",
    "    return ', '.join(formatted_keywords)  # 쉼표(,)와 공백으로 구분된 키워드 문자열을 반환\n",
    "\n",
    "# 데이터에 대해 키워드 정리 적용\n",
    "for data in details:\n",
    "    data['키워드'] = format_authors(data['키워드'])  # 각 논문 데이터의 '키워드'에 format_authors 함수를 적용하여 키워드 문자열 정리\n",
    "\n",
    "# 결과 출력\n",
    "print(details)  # 키워드가 정리된 논문 데이터 리스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(details)  # details 리스트를 Pandas DataFrame으로 변환\n",
    "df1.head(10)  # DataFrame의 처음 10개 행을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info() # df1의 요약정보 출력력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(f'riss_crawling_{search_keyword}2.csv', index=False)  # DataFrame df1을 CSV 파일로 저장, 인덱스 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일 읽기\n",
    "file1 = pd.read_csv(f'riss_crawling_{search_keyword}.csv')  # 첫 번째 CSV 파일 경로\n",
    "file2 = pd.read_csv(f'riss_crawling_{search_keyword}2.csv')  # 두 번째 CSV 파일 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1['제목'] = file1['제목'].str.strip().str.lower()  # file1의 '제목' 컬럼의 문자열 좌우 공백 제거 및 소문자 변환\n",
    "file2['제목'] = file2['제목'].str.strip().str.lower()  # file2의 '제목' 컬럼의 문자열 좌우 공백 제거 및 소문자 변환\n",
    "\n",
    "file1['제목'] = file1['제목'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())  # file1의 '제목' 컬럼의 문자열 내 여러 공백을 하나의 공백으로 대체하고 좌우 공백 제거\n",
    "file2['제목'] = file2['제목'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())  # file2의 '제목' 컬럼의 문자열 내 여러 공백을 하나의 공백으로 대체하고 좌우 공백 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Pandas 라이브러리 import\n",
    "\n",
    "# 공통 열(예: '제목')을 기준으로 DataFrame 병합\n",
    "merged_df = pd.merge(file1, file2, on='제목', how='inner')  # '제목' 컬럼을 기준으로 file1과 file2를 내부 조인(inner join)하여 병합\n",
    "\n",
    "merged_df.info()  # 병합된 DataFrame의 정보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(10) # 병합된 DatFrame의 처음 10개 행 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(f'merged_riss_crawling_{search_keyword}.csv', index=False) # merged_df DataFrame을 CSV 파일로 저장, 인덱스 제외"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
