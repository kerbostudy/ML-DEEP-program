{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DcNpP45pqgr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/diabetes.csv')\n",
        "X = df[df.columns[:-1]]\n",
        "y = df['Outcome']\n",
        "\n",
        "X = X.values\n",
        "y = torch.tensor(y.values)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "xNDXdZvQteRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ms = MinMaxScaler()\n",
        "ss = StandardScaler()\n",
        "X_train = ss.fit_transform(X_train)\n",
        "X_test = ss.transform(X_test)\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "y_train = ms.fit_transform(y_train)\n",
        "y_test = ms.transform(y_test)"
      ],
      "metadata": {
        "id": "WrI3HkU3ucKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)\n",
        "print(X_train, X_test)\n",
        "print(y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJK1meGVuzEJ",
        "outputId": "a65609c1-1852-4bee-d805-57b62ff3b2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(514, 8) (254, 8)\n",
            "(514, 1) (254, 1)\n",
            "[[ 1.89917664 -0.19475093  1.55717838 ... -1.04293476  1.60661512\n",
            "   0.06905859]\n",
            " [ 0.98286551  0.64046336 -0.55496107 ... -0.4148062   0.62609815\n",
            "   2.37289284]\n",
            " [ 0.06655438 -0.16381707  0.11203033 ... -1.29156898 -0.02953111\n",
            "   0.32504017]\n",
            " ...\n",
            " [ 1.89917664 -0.62782501  0.89018698 ...  1.78364376  1.94028358\n",
            "   0.41036736]\n",
            " [-1.1551938   0.6095295  -3.88991811 ...  1.36489138 -0.78467552\n",
            "  -0.35757739]\n",
            " [-1.1551938   0.11458769  1.44601315 ... -1.23922494 -0.61784129\n",
            "  -1.04019494]] [[ 0.67742846 -0.7206266  -0.66612631 ...  0.2656664  -0.12611934\n",
            "   0.83700334]\n",
            " [-0.54431971 -0.28755252  0.27877819 ...  0.4881286  -0.95150975\n",
            "  -1.04019494]\n",
            " [-0.54431971 -0.41128797 -0.3326306  ... -0.15308597 -0.92224058\n",
            "  -1.04019494]\n",
            " ...\n",
            " [-0.84975676 -3.75214514  0.22319557 ... -0.55875233 -0.50954538\n",
            "  -1.04019494]\n",
            " [ 1.28830255  0.67139722 -0.22146537 ...  0.38344051 -1.00712116\n",
            "   0.66634895]\n",
            " [-0.54431971 -0.62782501 -0.66612631 ... -1.01676274  0.41243326\n",
            "  -0.86954055]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# minmaxscaler only\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "minMaxScaler = MinMaxScaler()\n",
        "minMaxScaler.fit(X_train)\n",
        "tran_data_minMaxScaled = minMaxScaler.transform(X_train)"
      ],
      "metadata": {
        "id": "YQn_w7JQvtkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tran_data_minMaxScaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaO4xR93wX4A",
        "outputId": "243d90c3-595e-4291-e3f2-44d0f98ef269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.58823529 0.57788945 0.80327869 ... 0.35767511 0.4030743  0.21666667]\n",
            " [0.41176471 0.71356784 0.49180328 ... 0.42921013 0.26003416 0.66666667]\n",
            " [0.23529412 0.58291457 0.59016393 ... 0.32935917 0.16438941 0.26666667]\n",
            " ...\n",
            " [0.58823529 0.50753769 0.70491803 ... 0.67958271 0.45175064 0.28333333]\n",
            " [0.         0.70854271 0.         ... 0.6318927  0.05422716 0.13333333]\n",
            " [0.         0.6281407  0.78688525 ... 0.33532042 0.07856533 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# StandardScaler only\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "standardScaler = StandardScaler()\n",
        "standardScaler.fit(X_train)\n",
        "tran_data_standardScaled = standardScaler.transform(X_train)"
      ],
      "metadata": {
        "id": "exD0PZyGxELX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tran_data_standardScaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-BHjoQIxP6h",
        "outputId": "2d131452-0cf1-4464-af50-234b684c2d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.89917664 -0.19475093  1.55717838 ... -1.04293476  1.60661512\n",
            "   0.06905859]\n",
            " [ 0.98286551  0.64046336 -0.55496107 ... -0.4148062   0.62609815\n",
            "   2.37289284]\n",
            " [ 0.06655438 -0.16381707  0.11203033 ... -1.29156898 -0.02953111\n",
            "   0.32504017]\n",
            " ...\n",
            " [ 1.89917664 -0.62782501  0.89018698 ...  1.78364376  1.94028358\n",
            "   0.41036736]\n",
            " [-1.1551938   0.6095295  -3.88991811 ...  1.36489138 -0.78467552\n",
            "  -0.35757739]\n",
            " [-1.1551938   0.11458769  1.44601315 ... -1.23922494 -0.61784129\n",
            "  -1.04019494]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# customdataset\n",
        "class customdataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.len = len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "metadata": {
        "id": "79_GCgVIzwym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = customdataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "test_data = customdataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "fFLxZJ740SI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class binaryClassification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(binaryClassification, self).__init__()\n",
        "    self.layer_1 == nn.Linear(8, 64, bias=True)\n",
        "    self.layer_2 == nn.Linear(64, 64, bias=True)\n",
        "    self.layer_out = nn.Linear(64, 1, bias=True)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "    self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "    self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    x = self.relu(self.layer_1(inputs))\n",
        "    x = self.batchnorm1(x)\n",
        "    x = self.relu(self.layer_2(x))\n",
        "    x = self.batchnorm2(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer_out(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "it_IrHt61ClQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class binaryClassification(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(binaryClassification, self).__init__()\n",
        "    self.layer_1 = nn.Linear(8, 64, bias=True) # Changed == to =\n",
        "    self.layer_2 = nn.Linear(64, 64, bias=True) # Changed == to =\n",
        "    self.layer_out = nn.Linear(64, 1, bias=True)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "    self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "    self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    x = self.relu(self.layer_1(inputs))\n",
        "    x = self.batchnorm1(x)\n",
        "    x = self.relu(self.layer_2(x))\n",
        "    x = self.batchnorm2(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer_out(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "PLdFQyb52ape"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000 + 1\n",
        "print_epochs = 100\n",
        "learning_rate = 1e-2\n",
        "\n",
        "model = binaryClassification()\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "BCE = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z0kotEz1mT9",
        "outputId": "674eb74d-f7e2-4d59-b113-3941715a1a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binaryClassification(\n",
            "  (layer_1): Linear(in_features=8, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y_test):\n",
        "  y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "  correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "  acc = correct_results_sum / y_test.shape[0]\n",
        "  acc = torch.round(acc * 100)\n",
        "\n",
        "  return acc"
      ],
      "metadata": {
        "id": "BQmJGsKn2km9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  iteration_loss = 0\n",
        "  iteration_accuracy = 0\n",
        "\n",
        "  model.train()\n",
        "  for i, data in enumerate(train_loader):\n",
        "    X, y = data\n",
        "    y_pred = model(X.float())\n",
        "    loss = BCE(y_pred, y.reshape(-1,1).float())\n",
        "\n",
        "    iteration_loss += loss\n",
        "    iteration_accuracy += accuracy(y_pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if epoch % print_epochs == 0:\n",
        "    print('Train: epoch: {0} -loss: {1:.5f}; acc:{2:.3f}'.format(epoch, iteration_loss/(i+1), iteration_accuracy/(i+1)))\n",
        "\n",
        "  iteration_loss = 0\n",
        "  iteration_accuracy = 0\n",
        "  model.eval()\n",
        "  for i, data in enumerate(test_loader):\n",
        "    X, y = data\n",
        "    y_pred = model(X.float())\n",
        "    loss = BCE(y_pred, y.reshape(-1,1).float())\n",
        "    iteration_loss += loss\n",
        "    iteration_accuracy += accuracy(y_pred, y)\n",
        "  if (epoch % print_epochs == 0):\n",
        "    print('Test: epoch: {0} -loss: {1:.5f}; acc:{2:.3f}'.format(epoch, iteration_loss/(i+1), iteration_accuracy/(i+1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVghn-x32wSN",
        "outputId": "051e8e3c-5edd-4123-d1af-aca17a69d12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch: 0 -loss: 0.30243; acc:86.444\n",
            "Test: epoch: 0 -loss: 0.58675; acc:74.000\n",
            "Train: epoch: 100 -loss: 0.46001; acc:74.667\n",
            "Test: epoch: 100 -loss: 0.59032; acc:71.250\n",
            "Train: epoch: 200 -loss: 0.30321; acc:85.778\n",
            "Test: epoch: 200 -loss: 0.60104; acc:71.750\n",
            "Train: epoch: 300 -loss: 0.31729; acc:87.556\n",
            "Test: epoch: 300 -loss: 0.63605; acc:70.000\n",
            "Train: epoch: 400 -loss: 0.40223; acc:78.889\n",
            "Test: epoch: 400 -loss: 0.63640; acc:72.500\n",
            "Train: epoch: 500 -loss: 0.37140; acc:81.444\n",
            "Test: epoch: 500 -loss: 0.62745; acc:71.250\n",
            "Train: epoch: 600 -loss: 0.29138; acc:86.889\n",
            "Test: epoch: 600 -loss: 0.60483; acc:72.500\n",
            "Train: epoch: 700 -loss: 0.27713; acc:87.444\n",
            "Test: epoch: 700 -loss: 0.58458; acc:74.000\n",
            "Train: epoch: 800 -loss: 0.34358; acc:81.667\n",
            "Test: epoch: 800 -loss: 0.62848; acc:73.000\n",
            "Train: epoch: 900 -loss: 0.29447; acc:87.889\n",
            "Test: epoch: 900 -loss: 0.60587; acc:73.500\n",
            "Train: epoch: 1000 -loss: 0.52971; acc:80.556\n",
            "Test: epoch: 1000 -loss: 0.59668; acc:74.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  epoch_loss = 0  # Changed variable name for clarity\n",
        "  epoch_acc = 0   # Changed variable name for clarity\n",
        "\n",
        "  model.train()\n",
        "  for i, data in enumerate(train_loader, 0):  # Start enumerate from 0 for clarity\n",
        "    X, y = data\n",
        "    y_pred = model(X.float())\n",
        "    loss = BCE(y_pred, y.reshape(-1, 1).float())\n",
        "\n",
        "    epoch_loss += loss.item()  # Accumulate loss values\n",
        "    epoch_acc += accuracy(y_pred, y).item()  # Accumulate accuracy values\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if epoch % print_epochs == 0:\n",
        "    print(f'Train: epoch: {epoch} - loss: {epoch_loss/(i+1):.5f}; acc: {epoch_acc/(i+1):.3f}')  # Calculate and print average loss and accuracy\n",
        "\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "    for i, data in enumerate(test_loader, 0):  # Start enumerate from 0 for clarity\n",
        "      X, y = data\n",
        "      y_pred = model(X.float())\n",
        "      loss = BCE(y_pred, y.reshape(-1, 1).float())\n",
        "      epoch_loss += loss.item()  # Accumulate loss values\n",
        "      epoch_acc += accuracy(y_pred, y).item()  # Accumulate accuracy values\n",
        "\n",
        "  if epoch % print_epochs == 0:\n",
        "    print(f'Test: epoch: {epoch} - loss: {epoch_loss/(i+1):.5f}; acc: {epoch_acc/(i+1):.3f}')  # Calculate and print average loss and accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMN-9F1Z5yZN",
        "outputId": "1e682432-04ec-4ee6-969a-ae571dceb774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch: 0 - loss: 0.55946; acc: 75.111\n",
            "Test: epoch: 0 - loss: 0.60232; acc: 70.000\n",
            "Train: epoch: 100 - loss: 0.50560; acc: 72.333\n",
            "Test: epoch: 100 - loss: 0.56214; acc: 74.500\n",
            "Train: epoch: 200 - loss: 0.34937; acc: 85.556\n",
            "Test: epoch: 200 - loss: 0.54919; acc: 75.750\n",
            "Train: epoch: 300 - loss: 0.37595; acc: 82.000\n",
            "Test: epoch: 300 - loss: 0.78106; acc: 66.000\n",
            "Train: epoch: 400 - loss: 0.33970; acc: 84.000\n",
            "Test: epoch: 400 - loss: 0.57679; acc: 75.000\n",
            "Train: epoch: 500 - loss: 0.35969; acc: 84.111\n",
            "Test: epoch: 500 - loss: 0.58997; acc: 71.750\n",
            "Train: epoch: 600 - loss: 0.38394; acc: 78.444\n",
            "Test: epoch: 600 - loss: 0.53627; acc: 74.000\n",
            "Train: epoch: 700 - loss: 0.39065; acc: 83.889\n",
            "Test: epoch: 700 - loss: 0.52902; acc: 76.500\n",
            "Train: epoch: 800 - loss: 0.34163; acc: 85.111\n",
            "Test: epoch: 800 - loss: 0.53598; acc: 74.500\n",
            "Train: epoch: 900 - loss: 0.39608; acc: 83.667\n",
            "Test: epoch: 900 - loss: 0.54968; acc: 73.750\n",
            "Train: epoch: 1000 - loss: 0.35275; acc: 85.333\n",
            "Test: epoch: 1000 - loss: 0.54753; acc: 74.250\n"
          ]
        }
      ]
    }
  ]
}